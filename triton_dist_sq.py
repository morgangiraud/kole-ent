"""
This file has been auto generated by triton and then commented and edited.
"""
import torch
from torch import empty_strided
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.ir import ReductionHint
from torch._inductor.triton_heuristics import reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_cuda_stream
import triton
import triton.language as tl

from utils import get_sm_limits, BLOCK_MAX_NB_THREADS, WARP_SIZE
from ent import compute_dist_sq


assert_size_stride = torch._C._dynamo.guards.assert_size_stride


@triton.jit
def compute_dist_sq_triton_(in_ptr0, out_ptr0, N, D, xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):
    """
    This function works from the output perspective.
    The goal is to compute every pairwise distance between each row of in_ptr0
    and store this value in out_ptr0 for all the NxN values

    xnumel = N*N
    rnumel = D
    """
    # Compute output values offset
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]  # XBLOCK, 1

    # We guard against going out of the output matrix number of elements
    # This guard will also be used to avoid loading already computed output values
    xmask = xindex < xnumel

    # First trick:
    # This is nice way map output values to input indexes and compute all the pairwise distances
    # While ensuring we never go beyond the number of lines of the input matrix
    pw_row1_index = D * (xindex // N)  # [0, ..., 0, 1, ...]
    pw_row2_index = D * (xindex % N)  # [0, 1, ..., N-1, 0, ...]

    # We allocate the memory to store the temporary values
    acc_add = tl.full([XBLOCK, RBLOCK], 0, tl.float32)

    # Dynamic reduction working per block
    rbase = tl.arange(0, RBLOCK)[None, :]  # 1, RBLOCK
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel  # We guard against going out of the first dimension

        # Second trick
        # We use an outer AND and outer + operations to get the current reduction indexes for
        # the rows handled by the current block
        mask = rmask & xmask  # XBLOCK, RBLOCK
        in1_ptrs = pw_row1_index + rindex  # XBLOCK, RBLOCK
        in0_ptrs = pw_row2_index + rindex  # XBLOCK, RBLOCK

        data0 = tl.load(in_ptr0 + in1_ptrs, mask, eviction_policy="evict_last", other=0)
        data1 = tl.load(in_ptr0 + in0_ptrs, mask, eviction_policy="evict_last", other=0)

        # We do all the pointwise operations
        diff = data0 - data1
        diff_squared = diff * diff

        # This line is not needed because we are sure that we are working with
        # tensors of size [XBLOCK, RBLOCK] already
        # diff_squared_brodcasted = tl.broadcast_to(diff_squared, [XBLOCK, RBLOCK])

        # Those lines can be simplified because we mask our input values with the 0. value
        # and (0 - 0)**2 -> 0 so it won't interfere with the accumulation
        acc_add += diff_squared
        # # We add to our temporary buffer
        # # and make sure to only keep the values that has been updated
        # tmp6 = acc_add + diff_squared
        # acc_add = tl.where(mask, tmp6, acc_add)

    # We finally reduce to get final output values
    row_sum = tl.sum(acc_add, 1)[:, None]
    # And we write back in the global memory
    tl.store(out_ptr0 + xindex, row_sum, xmask)


def compute_dist_sq_triton(x):
    N, D = x.shape

    assert_size_stride(x, (N, D), (D, 1))

    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)  # no-op to ensure context

        out = empty_strided((N, N), (N, 1), device="cuda", dtype=torch.float32)

        # Working on a 1D grid
        xnumel = N * N
        rnumel = D

        # The goal of the config is to maximize the "occupancy" of the GPU
        # Occupancy = assigned warp to SM / SM maximum supported
        # So the goal is to ensure we use the maximum number of threads possible per SM at any point in time.

        # To do so, it is useful to know
        # - Maximum number of threads per block
        # - Maximum number of warps per SM
        # - Maximum number of block per SM
        # - Number of SM

        # Secondly it is important to not go beyond other caracteristiques like:
        # - register memory limit
        # - shared memory limit
        # - maximum memory bandwidth
        # - etc.
        # but in this case we hope that triton will do the work for us
        # and it will, as long as we provide to it enough information so
        # it can do its magic

        # Meta parameters heuristics
        num_warps = 2  # I don't understand why I get worst performance when increasing this value
        RBLOCK = min(triton.next_power_of_2(D), BLOCK_MAX_NB_THREADS)  #
        XBLOCK = BLOCK_MAX_NB_THREADS // RBLOCK
        assert XBLOCK * RBLOCK == BLOCK_MAX_NB_THREADS

        nb_blocks = 1 + (xnumel - 1) // XBLOCK
        g = (nb_blocks, 1, 1)

        compute_dist_sq_triton_[g](
            x,
            out,
            N,
            D,
            xnumel,
            rnumel,
            # Meta parameters
            XBLOCK=XBLOCK,
            RBLOCK=RBLOCK,
            num_warps=num_warps,
        )

        return out


if __name__ == "__main__":
    from utils import seed_everything

    seed_everything(0)

    print("Testing triton function...")
    for i in range(2, 12):
        x = torch.randn(129, 2**i + 1, device="cuda")

        y_triton = compute_dist_sq_triton(x)
        y_torch = compute_dist_sq(x)

        assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)
        print(f"input {2**i}: good")

    print("Triton function successfully tested!")
